{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv() # For OpenRouter API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RAG results\n",
    "import pandas as pd\n",
    "\n",
    "# Import csv file as dataframe\n",
    "df = pd.read_csv(CSV_FILE, sep=\";\") # Assumes that CSV file is in the same directory as this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "template = \"\"\"\n",
    "You will be provided with three pieces of text. One will be a question, one will be the Generated Answer that has been generated by a question answering system and the other will be the correct answer. You will be asked to determine if the generated answer is correct or not. Take the question into account. \n",
    "If the generated answer is correct, you will respond with just the digit 1. If the generated answer is incorrect, you will respond with just the digit 0. If you are unsure, you will respond with the digit 2. \n",
    "DO NOT respond with anything other than those digits. DO NOT add any additional comments or information.\n",
    "\n",
    "Question = {{question}}\n",
    "Generated Answer: {{generated_answer}}\n",
    "Correct Answer: {{correct_answer}}\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = PromptBuilder(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "\n",
    "MODEL_NAME = \"meta-llama/llama-3-70b-instruct:nitro\"\n",
    "\n",
    "generator = OpenAIGenerator(model=MODEL_NAME,\n",
    "                            api_key=Secret.from_env_var(\"OPENROUTER_API_KEY\"),\n",
    "                            api_base_url=\"https://openrouter.ai/api/v1\",\n",
    "                            generation_kwargs={\n",
    "                              \"temperature\": 0.0,\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "llm_pipeline = Pipeline()\n",
    "# Add components to your pipeline\n",
    "llm_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "llm_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "# Now, connect the components to each other\n",
    "llm_pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert overlap column to a string column\n",
    "df[\"overlap\"] = df[\"overlap\"].astype(str)\n",
    "# Make overlap column contain only \"\"\n",
    "df[\"overlap\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of rows in the DataFrame for the progress bar\n",
    "total_rows = len(df[(df[\"overlap\"] == \"\") | (df[\"overlap\"] == \"ERROR\") | (df[\"overlap\"].str.contains(\"<!DOCTYPE html>\", na=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Create a progress bar\n",
    "with tqdm(total=total_rows, desc=\"Processing rows\", dynamic_ncols=True) as pbar:\n",
    "    # Iterate over each row\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"overlap\"] != \"\" and row[\"overlap\"] != \"ERROR\" and not \"<!DOCTYPE html>\" in row[\"overlap\"]:\n",
    "            continue\n",
    "        \n",
    "        # Get the question and the context\n",
    "        question = row[\"questions\"]\n",
    "        generated_result = row[\"result\"]\n",
    "        correct_result = row[\"correct_result\"]\n",
    "        # Run the pipeline\n",
    "        try:\n",
    "            result = llm_pipeline.run({\"prompt_builder\": {\"question\": question, \"generated_answer\": generated_result, \"correct_answer\": correct_result}})[\"llm\"][\"replies\"][0]\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            result = \"ERROR\"\n",
    "        # Add to df\n",
    "        df.at[index, \"overlap\"] = result\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a new file\n",
    "df.to_csv(f\"eval_{CSV_FILE}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
